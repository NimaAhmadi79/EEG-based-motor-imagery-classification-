{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "import easygdf\n",
    "import mne\n",
    "import json\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Activation, MaxPooling1D, Dropout, Flatten\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import get_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from colabcode import ColabCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_position():\n",
    "\n",
    "    filenames = [\"A01T.gdf\", \"A01E.gdf\", \"A02T.gdf\", \"A02E.gdf\",\"A03T.gdf\",\"A03E.gdf\",\"A04T.gdf\",\"A04E.gdf\",\"A05T.gdf\",\"A05E.gdf\",\"A06T.gdf\",\"A06E.gdf\",\"A07T.gdf\",\"A07E.gdf\",\"A08T.gdf\",\"A08E.gdf\",\"A09T.gdf\",\"A09E.gdf\"]\n",
    "    labels = [\"A01T\", \"A01E\", \"A02T\", \"A02E\",\"A03T\",\"A03E\",\"A04T\",\"A04E\",\"A05T\",\"A05E\",\"A06T\",\"A06E\",\"A07T\",\"A07E\",\"A08T\",\"A08E\",\"A09T\",\"A09E\"]\n",
    "\n",
    "\n",
    "    events_with_labels_arrays = {}\n",
    "\n",
    "    for filename, label in zip(filenames, labels):\n",
    "\n",
    "        filename='../datasets/BCICIV_2a_gdf/'+filename\n",
    "        raw = mne.io.read_raw_gdf(filename)\n",
    "\n",
    "        # Access event information from the raw object\n",
    "        events, event_id = mne.events_from_annotations(raw)\n",
    "\n",
    "        #the list have the dictionary that map each events' id  to events' labels\n",
    "        events_id_labels = list(event_id.keys())\n",
    "\n",
    "        #create array for gather togther events' type and positions\n",
    "        temp= np.zeros_like(events)\n",
    "\n",
    "\n",
    "        for i in range(len(events)):\n",
    "            temp[i,0]=events[i,0]\n",
    "            id=events[i,2]\n",
    "            temp[i,1]=events_id_labels[id-1] # be in dalil ke dar list event ha ma id 10 drim vali araye events_id_labels k az tarigh key dastresi peyda krdim yedone adadash paiin tare\n",
    "\n",
    "        events_with_labels_arrays[label] = temp\n",
    "\n",
    "    \n",
    "    # Get the sample indices of the events\n",
    "    #event_positions_samples = events[:, 0]\n",
    "\n",
    "    # Get the corresponding time points from raw\n",
    "    #event_positions_times = raw.times[event_positions_samples]    \n",
    "\n",
    "    return events_with_labels_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_in_array():\n",
    "\n",
    "    filenames = [\"A01T.gdf\", \"A01E.gdf\", \"A02T.gdf\", \"A02E.gdf\",\"A03T.gdf\",\"A03E.gdf\",\"A04T.gdf\",\"A04E.gdf\",\"A05T.gdf\",\"A05E.gdf\",\"A06T.gdf\",\"A06E.gdf\",\"A07T.gdf\",\"A07E.gdf\",\"A08T.gdf\",\"A08E.gdf\",\"A09T.gdf\",\"A09E.gdf\"]\n",
    "    labels = [\"A01T\", \"A01E\", \"A02T\", \"A02E\",\"A03T\",\"A03E\",\"A04T\",\"A04E\",\"A05T\",\"A05E\",\"A06T\",\"A06E\",\"A07T\",\"A07E\",\"A08T\",\"A08E\",\"A09T\",\"A09E\"]\n",
    "\n",
    "\n",
    "    # Create a dictionary to store data arrays\n",
    "    data_arrays = {}\n",
    "\n",
    "\n",
    "    # Iterate over the filenames and labels\n",
    "    for filename, label in zip(filenames, labels):\n",
    "        # Read the raw data\n",
    "        filename='../datasets/BCICIV_2a_gdf/'+filename\n",
    "        raw = mne.io.read_raw_gdf(filename)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = raw.to_data_frame()\n",
    "        \n",
    "        # Store data array in the dictionary\n",
    "        data_arrays[label] = df.values  # Store the numpy array representation of the DataFrame\n",
    "\n",
    "    # # Access data array for a specific label\n",
    "    # example_label = \"A01T\"\n",
    "    # example_data_array = data_arrays[example_label]    \n",
    "\n",
    "    return data_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(data_arrays,events_with_labels_arrays):\n",
    "    labels = [\"A01T\", \"A02T\", \"A03T\",\"A04T\",\"A05T\",\"A06T\",\"A07T\",\"A08T\",\"A09T\"]\n",
    "\n",
    "    X_train=[]\n",
    "\n",
    "    for label in labels:\n",
    "\n",
    "        temp_data=data_arrays[label]\n",
    "        temp_events=events_with_labels_arrays[label]\n",
    "        for i in range(len(temp_events)):\n",
    "        \n",
    "            if temp_events[i,1]==769 or temp_events[i,1]==770 or temp_events[i,1]==771 or temp_events[i,1]==772 :\n",
    "\n",
    "                ev=temp_events[i,1]\n",
    "                position=temp_events[i,0]\n",
    "                for j in range(position, position+1000):\n",
    "\n",
    "                    temp=np.append(temp_data[j,1:23],ev)\n",
    "                    X_train.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the squeeze excitation block function\n",
    "def squeeze_excitation_block(inputs, r):\n",
    "    filters = inputs.shape[-1]\n",
    "    se = layers.GlobalAveragePooling1D()(inputs)\n",
    "    se = layers.Dense(filters // r, activation='relu')(se)\n",
    "    se = layers.Dense(filters, activation='sigmoid')(se)\n",
    "    se = layers.Reshape((1, filters))(se)\n",
    "    return layers.multiply([inputs, se])\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Define input shape\n",
    "input_shape = (22,1)\n",
    "\n",
    "# Define input layers for each branch\n",
    "input_1 = tf.keras.Input(shape=input_shape)\n",
    "input_2 = tf.keras.Input(shape=input_shape)\n",
    "input_3 = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "# First Branch\n",
    "x1 = layers.Conv1D(32, 20, strides=2, padding='same', kernel_regularizer=l2(0.01))(input_1)\n",
    "x1 = layers.BatchNormalization()(x1)\n",
    "x1 = layers.ELU()(x1)\n",
    "x1 = squeeze_excitation_block(x1, r=8)\n",
    "x1 = layers.MaxPooling1D()(x1)\n",
    "\n",
    "x1 = layers.Conv1D(32, 9, strides=2, padding='same', kernel_regularizer=l2(0.01))(x1)\n",
    "x1 = layers.BatchNormalization()(x1)\n",
    "x1 = layers.ELU()(x1)\n",
    "x1 = squeeze_excitation_block(x1, r=8)\n",
    "x1 = layers.MaxPooling1D()(x1)\n",
    "\n",
    "x1 = layers.Conv1D(32, 5, strides=1, padding='same', kernel_regularizer=l2(0.01))(x1)\n",
    "x1 = layers.BatchNormalization()(x1)\n",
    "x1 = layers.ELU()(x1)\n",
    "\n",
    "x1 = layers.Flatten()(x1)\n",
    "\n",
    "# Second Branch\n",
    "x2 = layers.Conv1D(32, 13, strides=2, padding='same', kernel_regularizer=l2(0.01))(input_2)\n",
    "x2 = layers.BatchNormalization()(x2)\n",
    "x2 = layers.ELU()(x2)\n",
    "x2 = squeeze_excitation_block(x2, r=8)\n",
    "\n",
    "x2 = layers.Conv1D(32, 7, strides=2, padding='same', kernel_regularizer=l2(0.01))(x2)\n",
    "x2 = layers.BatchNormalization()(x2)\n",
    "x2 = layers.ELU()(x2)\n",
    "x2 = squeeze_excitation_block(x2, r=8)\n",
    "\n",
    "x2 = layers.Conv1D(32, 5, strides=1, padding='same', kernel_regularizer=l2(0.01))(x2)\n",
    "x2 = layers.BatchNormalization()(x2)\n",
    "x2 = layers.ELU()(x2)\n",
    "\n",
    "x2 = layers.Flatten()(x2)\n",
    "\n",
    "# Third Branch\n",
    "x3 = layers.Conv1D(32, 15, strides=2, padding='same', kernel_regularizer=l2(0.01))(input_3)\n",
    "x3 = layers.BatchNormalization()(x3)\n",
    "x3 = layers.ELU()(x3)\n",
    "x3 = squeeze_excitation_block(x3, r=8)\n",
    "\n",
    "x3 = layers.Conv1D(32, 10, strides=2, padding='same', kernel_regularizer=l2(0.01))(x3)\n",
    "x3 = layers.BatchNormalization()(x3)\n",
    "x3 = layers.ELU()(x3)\n",
    "x3 = squeeze_excitation_block(x3, r=8)\n",
    "\n",
    "x3 = layers.Conv1D(32, 5, strides=1, padding='same', kernel_regularizer=l2(0.01))(x3)\n",
    "x3 = layers.BatchNormalization()(x3)\n",
    "x3 = layers.ELU()(x3)\n",
    "\n",
    "x3 = layers.Flatten()(x3)\n",
    "\n",
    "# Concatenate the outputs of three branches\n",
    "merged = layers.concatenate([x1, x2, x3])\n",
    "\n",
    "# Dropout layer\n",
    "x = layers.Dropout(0.5)(merged)\n",
    "\n",
    "# # Flatten layer\n",
    "# x = layers.Flatten()(x)\n",
    "\n",
    "# # Dense layer\n",
    "# x = layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "# Output layer\n",
    "output = layers.Dense(4, activation='softmax')(x)\n",
    "\n",
    "# Create model\n",
    "model = Model(inputs=[input_1, input_2, input_3], outputs=output)\n",
    "\n",
    "# Compile model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_with_labels_arrays=get_data.event_position()\n",
    "\n",
    "data_arrays=get_data.save_data_in_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add label for each row as last column\n",
    "train_data=get_data.create_training_data(data_arrays,events_with_labels_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert train_data to np array\n",
    "train_data = np.array(train_data)\n",
    "\n",
    "#Separate features (X) and labels (y)\n",
    "X = train_data[:, :-1]  # Extract all rows and all columns except the last one\n",
    "Y = train_data[:, -1]   # Extract all rows and only the last column\n",
    "\n",
    "#reshape Y to 2d array\n",
    "Y = Y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(Y)):\n",
    "    if Y[i]==769:\n",
    "        Y[i]=0\n",
    "    elif Y[i]==770:\n",
    "        Y[i]=1\n",
    "    elif Y[i]==771:\n",
    "        Y[i]=2    \n",
    "    elif Y[i]==772:\n",
    "        Y[i]=3    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_, y_train, y_ =train_test_split(X, Y, test_size=0.20, random_state=1)\n",
    "\n",
    "# x_cv, x_test, y_cv, y_test =train_test_split(x_, y_, test_size=0.50, random_state=1)\n",
    "\n",
    "# del x_ ,y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate shuffled indices\n",
    "indices = np.arange(len(x_train))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Shuffle x_train and y_train using the shuffled indices\n",
    "x_train_shuffled = x_train[indices]\n",
    "y_train_shuffled = y_train[indices]\n",
    "\n",
    "# Now x_train_shuffled and y_train_shuffled are shuffled while maintaining correspondence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history=model.fit([x_train_shuffled, x_train_shuffled, x_train_shuffled], y_train_shuffled, epochs=400, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Plot the training loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "okernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27dce945bb2da940a5b3e3764ba67512b0d4323c270d398d6a5469083993861b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

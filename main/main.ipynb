{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "import easygdf\n",
    "import mne\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Activation, MaxPooling1D, Dropout, Flatten\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import get_data\n",
    "from earlystopclass import EarlyStopAtAccuracy\n",
    "from ipynb.fs.full.data_preprocess import save_data_in_csv,get_all_info,get_event_position,save_data_in_array,combine_data_labels,interpolation,cropping,events_to_classes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from colabcode import ColabCode\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import Counter\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the squeeze excitation block function\n",
    "def squeeze_excitation_block(inputs, r):\n",
    "    filters = inputs.shape[-1]\n",
    "    se = layers.GlobalAveragePooling1D()(inputs)\n",
    "    se = layers.Dense(filters // r, activation='relu')(se)\n",
    "    se = layers.Dense(filters, activation='sigmoid')(se)\n",
    "    se = layers.Reshape((1, filters))(se)\n",
    "    out=inputs * se\n",
    "    return out\n",
    "\n",
    "# def SqueezeAndExcitation(inputs, ratio=8): \n",
    "#     b, _, _, c = inputs.shape \n",
    "#     x = GlobalAveragePooling2D()(inputs) \n",
    "#     x = Dense(c//ratio, activation=\"relu\", use_bias=False)(x) \n",
    "#     x = Dense(c, activation=\"sigmoid\", use_bias=False)(x) \n",
    "#     x = inputs * x \n",
    "#     return x \n",
    "\n",
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "# Define input shape\n",
    "input_shape = (1000,22)\n",
    "\n",
    "# Define input layers for each branch\n",
    "input_1 = tf.keras.Input(shape=input_shape)\n",
    "input_2 = tf.keras.Input(shape=input_shape)\n",
    "input_3 = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "# First Branch\n",
    "x1 = tf.keras.layers.Conv1D(32, 20, strides=2, padding='same', kernel_regularizer=l2(0.01))(input_1)\n",
    "x1 = tf.keras.layers.BatchNormalization()(x1)\n",
    "x1 = tf.keras.layers.ELU()(x1)\n",
    "x1 = squeeze_excitation_block(x1, r=8)\n",
    "x1 = tf.keras.layers.MaxPooling1D()(x1)\n",
    "\n",
    "\n",
    "x1 = tf.keras.layers.Conv1D(32, 9, strides=2, padding='same', kernel_regularizer=l2(0.01))(x1)\n",
    "x1 = tf.keras.layers.BatchNormalization()(x1)\n",
    "x1 = tf.keras.layers.ELU()(x1)\n",
    "x1 = squeeze_excitation_block(x1, r=8)\n",
    "x1 = tf.keras.layers.MaxPooling1D()(x1)\n",
    "skip_connection1 = x1 ############\n",
    "\n",
    "x1 = tf.keras.layers.Conv1D(32, 5, strides=1, padding='same', kernel_regularizer=l2(0.01))(x1)\n",
    "x1 = tf.keras.layers.BatchNormalization()(x1)\n",
    "x1 = tf.keras.layers.ELU()(x1)\n",
    "\n",
    "x1 = tf.keras.layers.Conv1D(32, 5, strides=1, padding='same', kernel_regularizer=l2(0.01))(x1)\n",
    "x1 = tf.keras.layers.BatchNormalization()(x1)\n",
    "x1 = tf.keras.layers.ELU()(x1)\n",
    "x1 = squeeze_excitation_block(x1, r=8)\n",
    "x1 = tf.keras.layers.add([x1, skip_connection1])############\n",
    "x1 = tf.keras.layers.MaxPooling1D()(x1)\n",
    "skip_connection2 = x1 ############\n",
    "\n",
    "x1 = tf.keras.layers.Conv1D(32, 3, strides=1, padding='same', kernel_regularizer=l2(0.01))(x1)\n",
    "x1 = tf.keras.layers.BatchNormalization()(x1)\n",
    "x1 = tf.keras.layers.ELU()(x1)\n",
    "\n",
    "x1 = tf.keras.layers.Conv1D(32, 3, strides=1, padding='same', kernel_regularizer=l2(0.01))(x1)\n",
    "x1 = tf.keras.layers.BatchNormalization()(x1)\n",
    "x1 = tf.keras.layers.ELU()(x1)\n",
    "x1 = squeeze_excitation_block(x1, r=8)\n",
    "x1 = tf.keras.layers.add([x1, skip_connection2])############\n",
    "x1 = tf.keras.layers.MaxPooling1D()(x1)\n",
    "x1 = tf.keras.layers.Dropout(0.5)(x1)\n",
    "\n",
    "x1 = tf.keras.layers.Flatten()(x1)\n",
    "\n",
    "# Second Branch\n",
    "x2 = tf.keras.layers.Conv1D(32, 13, strides=2, padding='same', kernel_regularizer=l2(0.01))(input_2)\n",
    "x2 = tf.keras.layers.BatchNormalization()(x2)\n",
    "x2 = tf.keras.layers.ELU()(x2)\n",
    "x2 = squeeze_excitation_block(x2, r=8)\n",
    "\n",
    "x2 = tf.keras.layers.Conv1D(32, 7, strides=2, padding='same', kernel_regularizer=l2(0.01))(x2)\n",
    "x2 = tf.keras.layers.BatchNormalization()(x2)\n",
    "x2 = tf.keras.layers.ELU()(x2)\n",
    "x2 = squeeze_excitation_block(x2, r=8)\n",
    "x2 = tf.keras.layers.MaxPooling1D()(x2)\n",
    "\n",
    "x2 = tf.keras.layers.Conv1D(32, 5, strides=1, padding='same', kernel_regularizer=l2(0.01))(x2)\n",
    "x2 = tf.keras.layers.BatchNormalization()(x2)\n",
    "x2 = tf.keras.layers.ELU()(x2)\n",
    "x2 = squeeze_excitation_block(x2, r=8)\n",
    "x2 = tf.keras.layers.MaxPooling1D()(x2)\n",
    "skip_connection3 = x2\n",
    "\n",
    "x2 = tf.keras.layers.Conv1D(32, 3, strides=1, padding='same', kernel_regularizer=l2(0.01))(x2)\n",
    "x2 = tf.keras.layers.BatchNormalization()(x2)\n",
    "x2 = tf.keras.layers.ELU()(x2)\n",
    "\n",
    "x2 = tf.keras.layers.Conv1D(32, 3, strides=1, padding='same', kernel_regularizer=l2(0.01))(x2)\n",
    "x2 = tf.keras.layers.BatchNormalization()(x2)\n",
    "x2 = tf.keras.layers.ELU()(x2)\n",
    "x2 = squeeze_excitation_block(x2, r=8)\n",
    "x2 = tf.keras.layers.add([x2, skip_connection3])############\n",
    "x2 = tf.keras.layers.MaxPooling1D()(x2)\n",
    "\n",
    "x2 = tf.keras.layers.Dropout(0.5)(x2)\n",
    "\n",
    "x2 = tf.keras.layers.Flatten()(x2)\n",
    "\n",
    "# Third Branch\n",
    "x3 = tf.keras.layers.Conv1D(32, 15, strides=2, padding='same', kernel_regularizer=l2(0.01))(input_3)\n",
    "x3 = tf.keras.layers.BatchNormalization()(x3)\n",
    "x3 = tf.keras.layers.ELU()(x3)\n",
    "x3 = squeeze_excitation_block(x3, r=8)\n",
    "x3 = tf.keras.layers.MaxPooling1D()(x3)\n",
    "\n",
    "x3 = tf.keras.layers.Conv1D(32, 10, strides=2, padding='same', kernel_regularizer=l2(0.01))(x3)\n",
    "x3 = tf.keras.layers.BatchNormalization()(x3)\n",
    "x3 = tf.keras.layers.ELU()(x3)\n",
    "x3 = squeeze_excitation_block(x3, r=8)\n",
    "\n",
    "x3 = tf.keras.layers.Conv1D(32, 5, strides=1, padding='same', kernel_regularizer=l2(0.01))(x3)\n",
    "x3 = tf.keras.layers.BatchNormalization()(x3)\n",
    "x3 = tf.keras.layers.ELU()(x3)\n",
    "x3 = squeeze_excitation_block(x3, r=8)\n",
    "x3 = tf.keras.layers.MaxPooling1D()(x3)\n",
    "\n",
    "x3 = tf.keras.layers.Conv1D(48, 3, strides=1, padding='same', kernel_regularizer=l2(0.01))(x3)\n",
    "x3 = tf.keras.layers.BatchNormalization()(x3)\n",
    "x3 = tf.keras.layers.ELU()(x3)\n",
    "x3 = squeeze_excitation_block(x3, r=8)\n",
    "x3 = tf.keras.layers.MaxPooling1D()(x3)\n",
    "\n",
    "x3 = tf.keras.layers.Dropout(0.5)(x3)\n",
    "\n",
    "x3 = tf.keras.layers.Flatten()(x3)\n",
    "\n",
    "# Concatenate the outputs of three branches\n",
    "merged = tf.keras.layers.concatenate([x1, x2, x3])\n",
    "\n",
    "\n",
    "#fc1 = tf.keras.layers.Dense(128, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.001))(merged)\n",
    "#fc2 = tf.keras.layers.Dense(64, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.001))(fc1)\n",
    "\n",
    "# Dropout layer\n",
    "x = tf.keras.layers.Dropout(0.5)(merged)\n",
    "\n",
    "# Output layer\n",
    "output = tf.keras.layers.Dense(4, activation='softmax')(x)\n",
    "\n",
    "# Create model\n",
    "model = Model(inputs=[input_1, input_2, input_3], outputs=output)\n",
    "\n",
    "# Compile model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine data recorded from subjects with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get info of events' positions\n",
    "positions_of_labels=get_event_position()\n",
    "\n",
    "#save raw eeg data recorded from subjects in array\n",
    "data_arrays=save_data_in_array()\n",
    "\n",
    "#combine data features or samples with their actual labels\n",
    "combined_data=combine_data_labels(data_arrays,positions_of_labels)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seprate test data before perform any data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate shuffled indices\n",
    "indices = np.arange(len(combined_data))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Shuffle x_train and y_train using the shuffled indices\n",
    "shuffled_combined_data = [combined_data[i] for i in indices]\n",
    "\n",
    "#seprate 12 percent of data for testing the model\n",
    "raw_train,raw_test =train_test_split(shuffled_combined_data, test_size=0.12, random_state=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seprate labels from raw_train and raw_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#seprate labels from feature data\n",
    "raw_x_train,raw_y_train = zip(*raw_train)\n",
    "x_test,y_test = zip(*raw_test)\n",
    "\n",
    "#convert type of array to numpy array\n",
    "raw_x_train=np.array(raw_x_train)\n",
    "raw_y_train=np.array(raw_y_train)\n",
    "\n",
    "x_test=np.array(x_test)\n",
    "y_test=np.array(y_test)\n",
    "\n",
    "#change shape of y_test and y_train beacuse this is better shape more practical and \n",
    "raw_y_train = raw_y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform interpolation operation and increase the size of matrices of each samples\n",
    "interpolated_raw_x_train=interpolation(raw_x_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cropping(extract new data from the primary data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform cropping operation to increase the size of the data by 6 times\n",
    "cropped_segments,segment_labels=cropping(interpolated_raw_x_train,raw_y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate new data to primary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#concatenate new created data from cropping operation to primary data\n",
    "x_train = np.concatenate((raw_x_train, cropped_segments), axis=0)\n",
    "y_train = np.concatenate((raw_y_train, segment_labels), axis=0)\n",
    "\n",
    "\n",
    "# shuffle x_train and y_train using the shuffled indices\n",
    "new_indices = np.arange(len(x_train))\n",
    "np.random.shuffle(new_indices)\n",
    "\n",
    "x_train = x_train[new_indices]\n",
    "y_train = y_train[new_indices]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change events' number to 0-3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cahnge events' number to 0-3 classes for y test/train\n",
    "y_test=events_to_classes(y_test)\n",
    "y_train=events_to_classes(y_train)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split train data to train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training data into training and cross-validation sets\n",
    "x_train, x_cv , y_train, y_cv =train_test_split(x_train, y_train, test_size=0.10, random_state=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"model_checkpoint.h5\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "#early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
    "early_stopping = EarlyStopAtAccuracy(monitor='val_accuracy', target_accuracy=0.93, patience=0, verbose=1, mode='max')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    [x_train, x_train, x_train], y_train,\n",
    "    epochs=60,\n",
    "    batch_size=64,\n",
    "    validation_data=([x_cv, x_cv, x_cv], y_cv),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Plot training and validation loss.\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cropping(interpolated_x_test,old_x_test):\n",
    "    # Initialize empty lists to store cropped segments and corresponding labels\n",
    "    increased_x_test = []\n",
    "    # Iterate over each sample\n",
    "    for sample_idx in range(interpolated_x_test.shape[0]):\n",
    "        # Get the EEG data for the current sample\n",
    "        sample_data = interpolated_x_test[sample_idx, :, :]\n",
    "        # Calculate the number of segments\n",
    "        num_segments = (sample_data.shape[0] - 1000) // 10 + 1\n",
    "\n",
    "        temp=[]\n",
    "        temp.append(old_x_test[sample_idx])\n",
    "        # Iterate over each segment\n",
    "        for segment_idx in range(num_segments):\n",
    "            # Calculate start index of the segment\n",
    "            start_idx = segment_idx * 10\n",
    "            # Extract the segment\n",
    "            segment = sample_data[start_idx:start_idx + 1000, :]\n",
    "            # Append the segment to the list of cropped segments\n",
    "            temp.append(segment)\n",
    "\n",
    "        increased_x_test.append(temp)\n",
    "    # Convert the lists to numpy arrays\n",
    "    increased_x_test = np.array(increased_x_test)\n",
    "    \n",
    "\n",
    "    return increased_x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_x_test=x_test\n",
    "original_y_test=y_test\n",
    "\n",
    "interpolated_x_test=interpolation(x_test)\n",
    "increased_x_test=test_cropping(interpolated_x_test,original_x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_y_pred=[]\n",
    "for i in range(increased_x_test.shape[0]):\n",
    "    y_predict=model.predict([increased_x_test[i], increased_x_test[i], increased_x_test[i]])\n",
    "    y_pred_labels = np.argmax(y_predict, axis=1)\n",
    "    counter = Counter(y_pred_labels)\n",
    "    most_common = counter.most_common(1)\n",
    "    final_y_pred.append(most_common[0][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_y_pred=np.array(final_y_pred)##\n",
    "final_y_pred=final_y_pred.reshape(-1,1)\n",
    "final_y_pred = final_y_pred.astype(np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 Score: 0.7264175568789091\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(y_test, final_y_pred, average='weighted')\n",
    "print(f'Test F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7275641025641025\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = accuracy_score(y_test, final_y_pred)\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "okernel",
   "language": "python",
   "name": "okernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
